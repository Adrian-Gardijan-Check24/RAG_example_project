{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports und Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "file_path = r\"teilnahmebed_text.txt\"\n",
    "api_key = \"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\"\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\", openai_api_key=api_key)\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textdatei einlesen und für Embedding vorbereiten, vorbereite gesäuberte Textdatei abspeichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_top_level_headers(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for i in range(len(lines)):\n",
    "        # Skip first two lines (they only contain headers etc)\n",
    "        if i < 2:\n",
    "            continue\n",
    "\n",
    "        # Detect a top-level section header (e.g., \"3\") followed by a short title\n",
    "        if re.match(r\"^\\d+$\", lines[i].strip()) and i + 1 < len(lines):\n",
    "            continue  # Do not include the number\n",
    "        elif re.match(r\"^\\d+$\", lines[i - 1].strip()):\n",
    "            continue  # Skip title (line after number)\n",
    "\n",
    "        # Add the rest of the content\n",
    "        cleaned_lines.append(lines[i])\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "\n",
    "def propagate_parent_sections(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Propagates parent sections to their respective child sections in a given text.\n",
    "    This function processes a text where sections are denoted by hierarchical numbering (e.g., \"4.2.\", \"4.2.1.\", \"4.2.1.1.\").\n",
    "    It appends the parent section titles to their respective child sections to provide context.\n",
    "    Args:\n",
    "        text (str): The input text containing sections and sub-sections.\n",
    "    Returns:\n",
    "        str: The processed text with parent sections propagated to their child sections.\n",
    "    \"\"\"\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "    current_parent = \"\"\n",
    "    current_child = \"\"\n",
    "    parent_coming = False\n",
    "    child_coming = False\n",
    "    childchild_coming = False\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        if parent_coming:\n",
    "            current_parent = stripped\n",
    "            parent_coming = False\n",
    "\n",
    "        if child_coming:\n",
    "            current_child = stripped\n",
    "            cleaned_lines.append(current_parent + \" \" + line)\n",
    "            child_coming = False\n",
    "        elif childchild_coming:\n",
    "            cleaned_lines.append(current_parent + \" \" + current_child + \" \" + line)\n",
    "            childchild_coming = False\n",
    "        else:\n",
    "            cleaned_lines.append(line)\n",
    "        \n",
    "        # Detect major section (e.g., \"4.2.\") but NOT \"4.2.1.\"\n",
    "        if re.match(r\"^\\d+\\.\\d+\\.$\", stripped):\n",
    "            parent_coming = True\n",
    "        \n",
    "        # Detect sub-sections (e.g., \"4.2.1.\") but not top-level sections\n",
    "        elif re.match(r\"^\\d+\\.\\d+\\.\\d+\\.$\", stripped):\n",
    "            child_coming = True\n",
    "\n",
    "            # Detect sub-sections (e.g., \"4.2.1.\") but not top-level sections\n",
    "        elif re.match(r\"^\\d+\\.\\d+\\.\\d+\\.\\d+\\.$\", stripped):\n",
    "            childchild_coming = True\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        if re.match(r\"^\\d+\\.\\d+\\.$\", line) or re.match(r\"^\\d+\\.\\d+\\.\\d+\\.$\", line) or re.match(r\"^\\d+\\.\\d+\\.\\d+\\.\\d+\\.$\", line):\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file cleaned and saved\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = remove_top_level_headers(text)\n",
    "text = propagate_parent_sections(text)\n",
    "text = remove_numbers(text)\n",
    "\n",
    "# Save the cleaned text (optional)\n",
    "with open(\"processed_terms.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(text)\n",
    "\n",
    "print(\"file cleaned and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text in Chunks aufteilen und Embedding-Vectors berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization complete! Stored 69 chunks in FAISS.\n"
     ]
    }
   ],
   "source": [
    "with open(\"processed_terms.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# Generate embeddings using OpenAI\n",
    "embeddings = embedding_model.embed_documents(chunks)\n",
    "\n",
    "# Convert to FAISS format with euclidian(L2) distance measure for similarity search\n",
    "dimension = len(embeddings[0])  # Get embedding size\n",
    "faiss_index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "faiss_index.add(np.array(embeddings, dtype=np.float32))\n",
    "\n",
    "# Store chunk text with same indices as in FAISS\n",
    "chunk_metadata = {i: chunks[i] for i in range(len(chunks))}\n",
    "\n",
    "# Save FAISS index (optional)\n",
    "faiss.write_index(faiss_index, \"terms_faiss.index\")\n",
    "\n",
    "print(\"Vectorization complete! Stored\", len(chunks), \"chunks in FAISS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für Prompt relevanteste Chunks heraussuchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(query, number_of_chunks_to_retrieve=5):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "\n",
    "    # Search for similar chunks\n",
    "    _, similar_indices = faiss_index.search(np.array([query_embedding], dtype=np.float32), \n",
    "                                            number_of_chunks_to_retrieve)\n",
    "\n",
    "    # Get the text of the similar chunks\n",
    "    similar_chunks = [chunk_metadata[i] for i in similar_indices[0]]\n",
    "\n",
    "    return similar_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell Testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(prompt):\n",
    "    answer = \"\"\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True, \n",
    "    )\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            answer += chunk.choices[0].delta.content\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frage:\n",
      " Was kann ich gewinnen?\n",
      "\n",
      "Kontext wird gesucht...\n",
      "\n",
      "Kontext 1:\n",
      "Gutscheine aus der Gesamtwertung und den jeweiligen Spieltagen im Wert von 240 € (in Worten zweihundertvierzig) und weniger werden passend zur Gewinneranzahl aufgestockt, damit jeder mit entsprechender Platzierung einen Gutschein erhält.\n",
      "Für die zu gewinnenden Gutscheine gelten die folgenden separaten Gutscheinbedingungen der CHECK24 GmbH (hier).\n",
      "Ein Anspruch auf Barauszahlung eines Gewinnes (nach Ziffer 4) besteht seitens des Gewinners nicht.\n",
      "Kontext 2:\n",
      "Pro Spieltag werden folgende Gewinne für die Teilnehmer mit den höchsten erreichten Punkten (nach Ziffer 2) der jeweiligen Spieltage vergeben: 24 x 240 € Pauschalreise-Gutscheine (einlösbar ab 2.400 € Buchungswert (exkl. lokaler Steuern, wie z.B. Touristensteuer))\n",
      "Pro Spieltag werden folgende Gewinne für alle Mitglieder der besten teilnahmeberechtigten Tipprunden (nach der Ziffer 4.2.), gemessen nach Durchschnittspunktzahl für den jeweiligen Spieltag: 24 x 10 % Hotel- & Ferienwohnungs-Gutschein\n",
      "Kontext 3:\n",
      "In der Gesamtwertung der CHECK24 Tipprunde vergibt der Veranstalter für die Nutzer mit den meisten Punkten (gem. Ziffer 2) die folgenden Gewinne:\n",
      "1 x 2.400 € Reise-Guthaben\n",
      "Zusätzlich gibt es mehrere vom Veranstalter definierte Spieltage. Ein Spieltag bezeichnet einen festgelegten Termin, an dem eine Runde von Spielen innerhalb des Wettbewerbs, wie in der Gruppenphase oder K.-o.-Runde, ausgetragen wird. An einem Spieltag finden mehrere Spiele gleichzeitig statt.\n",
      "Kontext 4:\n",
      "Preise gewinnen. Die Gutscheine (nach der Ziffer 4.4.) werden bei Gewinn automatisch dem CHECK24 Kundenkonto gutgeschrieben. Sollte ein Nutzer sein Konto löschen, greifen die Teilnahmebedingungen des CHECK24-Tippspiels (siehe Link) und der Nutzer verliert jeglichen Anspruch auf einen Gewinn (nach dieser Ziffer). Zudem können bei Verstößen gegen die Teilnahmebedingungen am CHECK24-Tippspiel auch nachträglich Gewinne aberkannt und ggf. zurückgefordert werden (siehe Link)\n",
      "Kontext 5:\n",
      "Die Vergabe der Preise (nach dieser Ziffer) erfolgt an die Teilnehmer, welche in der „CHECK24 Tipprunde“ (Zusammenschluss aller Teilnehmer am Gewinnspiel) die meisten Punkte erreicht haben. Zusätzlich können die bestplatzierten Teilnehmer in bestimmten Tipprunden (nach der Ziffer 4.2.) weitere Preise gewinnen. Die Gutscheine (nach der Ziffer 4.4.) werden bei Gewinn automatisch dem CHECK24 Kundenkonto gutgeschrieben. Sollte ein Nutzer sein Konto löschen, greifen die Teilnahmebedingungen des\n",
      "\n",
      "Antwort wird generiert...\n",
      "\n",
      "Antwort:\n",
      " In dem Gewinnspiel können verschiedene Preise gewonnen werden:\n",
      "\n",
      "1. **Pro Spieltag**:\n",
      "   - 24 x 240 € Pauschalreise-Gutscheine, die ab einem Buchungswert von 2.400 € einlösbar sind (exklusive lokaler Steuern).\n",
      "   - 24 x 10 % Hotel- & Ferienwohnungs-Gutscheine für Mitglieder der besten teilnahmeberechtigten Tipprunden, basierend auf der Durchschnittspunktzahl.\n",
      "\n",
      "2. **In der Gesamtwertung**:\n",
      "   - 1 x 2.400 € Reise-Guthaben für den Nutzer mit den meisten Punkten in der CHECK24 Tipprunde.\n",
      "\n",
      "Bitte beachten Sie, dass die Gutscheine automatisch dem CHECK24-Kundenkonto gutgeschrieben werden, und es keinen Anspruch auf eine Barauszahlung der Gewinne gibt.\n"
     ]
    }
   ],
   "source": [
    "# Hier eine Frage zu den Teilnahmebedingungen eingeben um das Modell zu testen\n",
    "query = \"Was kann ich gewinnen?\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Frage:\\n\", query)\n",
    "\n",
    "\n",
    "# Get context\n",
    "print(\"\\nKontext wird gesucht...\\n\")\n",
    "\n",
    "context_chunks = get_context(query, number_of_chunks_to_retrieve=5)\n",
    "\n",
    "print(\"\\n\".join([\"Kontext \" + str(i + 1) + \":\\n\" + context for i, context in enumerate(context_chunks)]))\n",
    "\n",
    "context = \"\\n\".join(context_chunks) # Combine chunks into one string\n",
    "\n",
    "\n",
    "# Create final prompt\n",
    "print(\"\\nAntwort wird generiert...\\n\")\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Du bist ein KI-Assistent, der Fragen zu den Teilnahmebedingungen eines Gewinnspiels beantwortet. Zur Beantwortung der Frage hast du folgenden Kontext:\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Die Frage des Benutzers lautet:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "answer = get_answer(prompt)\n",
    "\n",
    "print(\"Antwort:\\n\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
